{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bite08c0b10238845f5b3a369da6da33380",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MNIST dataset handling\n",
    "---\n",
    "\n",
    "In this notebook we will develop a custom `dataset` class which will be able to:\n",
    "- import the MNIST dataset from a **url**\n",
    "- **read** the MNIST dataset and **store** it in a `torch.tensor`\n",
    "- **save** the dataset in `.pt` format to be easily accessible within the `PyTorch` environment\n",
    "- provide a method to create the dataset **splits**, according to some proportions\n",
    "- provide a method to perform some **preprocessing** operations\n",
    "\n",
    "We will procede as follows:\n",
    "- file decoding procedure\n",
    "    - analisys of the MNIST dataset format (info taken from this [source](http://yann.lecun.com/exdb/mnist/))\n",
    "    - download the files from the sources\n",
    "    - read the file and retrieve the data dimensions and type\n",
    "    - store the data into a `torch.tensor` and save it to memory in `.pt` format\n",
    "- `dataset` class implementation\n",
    "    - define a constructor `__init__`\n",
    "    - provide a method `splits` to split the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset class construction\n",
    "\n",
    "> At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It represents a Python iterable over a dataset.\n",
    "\n",
    "The `DataLoader` class takes several arguments but the most important two are:\n",
    "- `Dataset`: an abstract class representing a dataset. All subclasses of `Dataset` should overwrite:\n",
    "    - `__getitem__()`: returns a fetched data sample for a given key\n",
    "    - `__len__()`: returns the size of the dataset\n",
    "- `sampler` or `batch_sampler`: they define the strategy to draw samples of batches of samples from the dataset.\n",
    "\n",
    "So, the first thing to consider is to develop a subclass of `torch.utils.data.Dataset` which overloads the aformentioned methods and implements the ones of which we talked in the `file_decoding_procedure` notebook.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Class constructor\n",
    "\n",
    "The class constructor must be able to:\n",
    "- download the dataset if requested and read it\n",
    "- create the tensors and do all the load/store stuff\n",
    "- leave the dataset empty if requested (it will be usefull to create splits)\n",
    "\n",
    "### Overloading\n",
    "\n",
    "The overloading of the `__len__()` and `__getitem__()` functions is straightforward.\n",
    "\n",
    "### Save the dataset\n",
    "\n",
    "We can use the already created function `save` to exploit this task.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# import torch\n",
    "\n",
    "# class MNIST(torch.utils.data.Dataset):\n",
    "\n",
    "#     def __init__(\n",
    "#           self\n",
    "#         , folder: str\n",
    "#         , train: bool\n",
    "#         , download: bool=False\n",
    "#         , empty: bool=False\n",
    "#         ) -> None:\n",
    "#         \"\"\"\n",
    "#         Class constructor.\n",
    "\n",
    "#         Args:\n",
    "#             folder (str): folder in which contains/will contain the data\n",
    "#             train (bool): if True the training dataset is built, otherwise the test dataset\n",
    "#             download (bool): if True the dataset will be downloaded (default = True)\n",
    "#             empty (bool): if True the tensors will be left empty (default = False)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # user folder check\n",
    "#         # ------------------------\n",
    "#         if folder is None:\n",
    "#             raise FileNotFoundError(\"Please specify the data folder\")\n",
    "#         if not os.path.exists(folder) or os.path.isfile(folder):\n",
    "#             raise FileNotFoundError(\"Invalid data path: {}\".format(folder))\n",
    "#         # ------------------------\n",
    "\n",
    "#         # utilities\n",
    "#         # ------------------------\n",
    "#         if train:\n",
    "#             urls = {\n",
    "#                 'training-images': 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "#                 , 'training-labels': 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "#             }\n",
    "#             self.save_file = 'training.pt'\n",
    "#         else:\n",
    "#             urls = {\n",
    "#                 'test-images': 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "#                 , 'test-labels': 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "#             }\n",
    "#             self.save_file = 'test.pt'\n",
    "#         # ------------------------\n",
    "\n",
    "#         # class members\n",
    "#         # ------------------------\n",
    "#         self.raw_folder = os.path.join(folder, \"data/raw\")\n",
    "#         self.processed_folder = os.path.join(folder, \"data/processed\")\n",
    "#         # ------------------------\n",
    "\n",
    "#         # dataset download\n",
    "#         # ------------------------\n",
    "#         if download:\n",
    "#             for name, url in urls.items():\n",
    "#                 utils.download(url, self.raw_folder, name)\n",
    "#         # ------------------------\n",
    "        \n",
    "#         # dataset folder check\n",
    "#         # ------------------------\n",
    "#         else:   # not download\n",
    "#             if not os.path.exists(self.raw_folder) or os.path.isfile(self.raw_folder):\n",
    "#                 raise FileNotFoundError(\"Invalid data path: {}\".format(self.raw_folder))\n",
    "#         # ------------------------\n",
    "\n",
    "#         # data storing\n",
    "#         # ------------------------\n",
    "#         if not empty:\n",
    "#             for name, _ in urls.items():\n",
    "#                 filepath = os.path.join(self.raw_folder, name)\n",
    "#                 if \"images\" in name:\n",
    "#                     self.data = utils.store_file_to_tensor(filepath)\n",
    "#                 elif \"labels\" in name:\n",
    "#                     self.labels = utils.store_file_to_tensor(filepath)\n",
    "#             self.save()\n",
    "            \n",
    "#         else:\n",
    "#             self.data = None\n",
    "#             self.labels = None\n",
    "#         # ------------------------\n",
    "            \n",
    "    \n",
    "#     def __len__(self) -> int:\n",
    "#         \"\"\"\n",
    "#         Return the lenght of the dataset.\n",
    "\n",
    "#         Returns:\n",
    "#             length of the dataset (int)\n",
    "#         \"\"\"\n",
    "#         return len(self.data) if self.data is not None else 0\n",
    "\n",
    "    \n",
    "#     def __getitem__(\n",
    "#           self\n",
    "#         , idx: int\n",
    "#         ) -> tuple:\n",
    "#         \"\"\"\n",
    "#         Retrieve the item of the dataset at index idx.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): index of the item to be retrieved.\n",
    "        \n",
    "#         Returns:\n",
    "#             tuple: (image, label) \n",
    "#         \"\"\"\n",
    "#         img, label = self.data[idx], int(self.labels[idx])\n",
    "\n",
    "#         return (img, label)\n",
    "    \n",
    "\n",
    "\n",
    "#     def save(self) -> None:\n",
    "#         \"\"\"\n",
    "#         Save the dataset (tuple of torch.tensors) into a file defined by self.processed_folder and self.save_file.\n",
    "#         \"\"\"\n",
    "#         if not os.path.exists(self.processed_folder):   \n",
    "#             os.makedirs(self.processed_folder)  \n",
    "\n",
    "#         # saving the training set into the correct folder\n",
    "#         with open(os.path.join(self.processed_folder, self.save_file), 'wb') as f:\n",
    "#             torch.save((self.data, self.labels), f)\n",
    "\n",
    "\n",
    "#     def load(self) -> None:\n",
    "#         \"\"\"\n",
    "#         Load the file .pt in the path defined by self.processed_folder and self.save_file.\n",
    "#         \"\"\"\n",
    "#         file_path = os.path.join(self.processed_folder, self.save_file)\n",
    "\n",
    "#         if not os.path.exists(file_path):\n",
    "#             raise FileNotFoundError(\"Folder not present: {}\".format(file_path))\n",
    "\n",
    "#         self.data, self.labels = torch.load(file_path)\n",
    "\n",
    "    \n",
    "#     def splits(\n",
    "#           self\n",
    "#         , proportions: list=[0.8, 0.2]\n",
    "#         , shuffle: bool=True\n",
    "#         ) -> None:\n",
    "#         \"\"\"\n",
    "#         Split the the dataset according to the given proportions and return two instances of MNIST, training and validation.\n",
    "\n",
    "#         Args:\n",
    "#             proportions (list): (default=[0.8,0.2]) list of proportions for training set and validation set.\n",
    "#             shuffle (bool): (default=True) whether to shuffle the dataset or not\n",
    "#         \"\"\"\n",
    "\n",
    "#         # check proportions\n",
    "#         # ------------------------\n",
    "#         if not (sum(proportions) == 1. and all([p > 0. for p in proportions])): #and len(proportions) == 2:\n",
    "#             raise ValueError(\"Invalid proportions: they must (1) be 2 (2) sum up to 1\") # (3) be all positives.\")\n",
    "#         # ------------------------\n",
    "\n",
    "#         # creating a list of MNIST objects\n",
    "#         # ------------------------\n",
    "#         datasets = []\n",
    "#         for i in range(len(proportions)):\n",
    "#             dataset.append(MNIST())\n",
    "#         # ------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading ./data/raw/training-images ...\n",
      "./data/raw/training-images loaded!\n",
      "Loading ./data/raw/training-labels ...\n",
      "./data/raw/training-labels loaded!\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "import torch\n",
    "\n",
    "path = \"./\"\n",
    "data_set = dataset.MNIST(path, train=True, download=False, empty=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = data_set.splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([48000, 28, 28])\ntorch.Size([48000])\n----------\ntorch.Size([12000, 28, 28])\ntorch.Size([12000])\n----------\n"
     ]
    }
   ],
   "source": [
    "for i in datasets:\n",
    "    print(i.data.shape)\n",
    "    print(i.labels.shape)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N. samples:    \t60000\nClasses:       \t{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\nClasses distr.: \ttensor([ 9.8717, 11.2367,  9.9300, 10.2183,  9.7367,  9.0350,  9.8633, 10.4417,\n         9.7517,  9.9150])\nData type:     \t<class 'torch.Tensor'>\nData shape:    \ttorch.Size([28, 28])\n\n"
     ]
    }
   ],
   "source": [
    "data_set.statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return [2,3]\n",
    "\n",
    "[a, b] = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.unsqueeze(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2055, 0.8854, 0.4279],\n",
       "        [0.4239, 0.9809, 0.5838],\n",
       "        [0.5909, 0.7128, 0.4041]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}