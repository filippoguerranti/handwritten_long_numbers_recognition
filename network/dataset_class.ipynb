{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitf5e98526d62c4af1bdda9906588672b8",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MNIST dataset handling\n",
    "---\n",
    "\n",
    "In this notebook we will develop a custom `dataset` class which will be able to:\n",
    "- import the MNIST dataset from a **url**\n",
    "- **read** the MNIST dataset and **store** it in a `torch.tensor`\n",
    "- **save** the dataset in `.pt` format to be easily accessible within the `PyTorch` environment\n",
    "- provide a method to create the dataset **splits**, according to some proportions\n",
    "- provide a method to perform some **preprocessing** operations\n",
    "\n",
    "We will procede as follows:\n",
    "- file decoding procedure\n",
    "    - analisys of the MNIST dataset format (info taken from this [source](http://yann.lecun.com/exdb/mnist/))\n",
    "    - download the files from the sources\n",
    "    - read the file and retrieve the data dimensions and type\n",
    "    - store the data into a `torch.tensor` and save it to memory in `.pt` format\n",
    "- `dataset` class implementation\n",
    "    - define a constructor `__init__`\n",
    "    - provide a method `splits` to split the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset class construction\n",
    "\n",
    "> At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It represents a Python iterable over a dataset.\n",
    "\n",
    "The `DataLoader` class takes several arguments but the most important two are:\n",
    "- `Dataset`: an abstract class representing a dataset. All subclasses of `Dataset` should overwrite:\n",
    "    - `__getitem__()`: returns a fetched data sample for a given key\n",
    "    - `__len__()`: returns the size of the dataset\n",
    "- `sampler` or `batch_sampler`: they define the strategy to draw samples of batches of samples from the dataset.\n",
    "\n",
    "So, the first thing to consider is to develop a subclass of `torch.utils.data.Dataset` which overloads the aformentioned methods and implements the ones of which we talked in the `file_decoding_procedure` notebook.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Class constructor\n",
    "\n",
    "The class constructor must be able to:\n",
    "- download the dataset if requested and read it\n",
    "- create the tensors and do all the load/store stuff\n",
    "- leave the dataset empty if requested (it will be usefull to create splits)\n",
    "\n",
    "### Overloading\n",
    "\n",
    "The overloading of the `__len__()` and `__getitem__()` functions is straightforward.\n",
    "\n",
    "### Save the dataset\n",
    "\n",
    "We can use the already created function `save` to exploit this task.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-12-5edbf41f802a>, line 139)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-5edbf41f802a>\"\u001b[0;36m, line \u001b[0;32m139\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import torch\n",
    "\n",
    "class MNIST(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "          self\n",
    "        , folder: str\n",
    "        , train: bool\n",
    "        , download: bool=False\n",
    "        , empty: bool=False\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        Args:\n",
    "            folder (str): folder in which contains/will contain the data\n",
    "            train (bool): if True the training dataset is built, otherwise the test dataset\n",
    "            download (bool): if True the dataset will be downloaded (default = True)\n",
    "            empty (bool): if True the tensors will be left empty (default = False)\n",
    "        \"\"\"\n",
    "\n",
    "        # user folder check\n",
    "        # ------------------------\n",
    "        if folder is None:\n",
    "            raise FileNotFoundError(\"Please specify the data folder\")\n",
    "        if not os.path.exists(folder) or os.path.isfile(folder):\n",
    "            raise FileNotFoundError(\"Invalid data path: {}\".format(folder))\n",
    "        # ------------------------\n",
    "\n",
    "        # utilities\n",
    "        # ------------------------\n",
    "        if train:\n",
    "            urls = {\n",
    "                'training-images': 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "                , 'training-labels': 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "            }\n",
    "            self.save_file = 'training.pt'\n",
    "        else:\n",
    "            urls = {\n",
    "                'test-images': 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "                , 'test-labels': 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            }\n",
    "            self.save_file = 'test.pt'\n",
    "        # ------------------------\n",
    "\n",
    "        # class members\n",
    "        # ------------------------\n",
    "        self.raw_folder = os.path.join(folder, \"data/raw\")\n",
    "        self.processed_folder = os.path.join(folder, \"data/processed\")\n",
    "        # ------------------------\n",
    "\n",
    "        # dataset download\n",
    "        # ------------------------\n",
    "        if download:\n",
    "            for name, url in urls.items():\n",
    "                utils.download(url, self.raw_folder, name)\n",
    "        # ------------------------\n",
    "        \n",
    "        # dataset folder check\n",
    "        # ------------------------\n",
    "        else:   # not download\n",
    "            if not os.path.exists(self.raw_folder) or os.path.isfile(self.raw_folder):\n",
    "                raise FileNotFoundError(\"Invalid data path: {}\".format(self.raw_folder))\n",
    "        # ------------------------\n",
    "\n",
    "        # data storing\n",
    "        # ------------------------\n",
    "        if not empty:\n",
    "            for name, _ in urls.items():\n",
    "                filepath = os.path.join(self.raw_folder, name)\n",
    "                if \"images\" in name:\n",
    "                    self.data = utils.store_file_to_tensor(filepath)\n",
    "                elif \"labels\" in name:\n",
    "                    self.labels = utils.store_file_to_tensor(filepath)\n",
    "            self.save()\n",
    "            \n",
    "        else:\n",
    "            self.data = None\n",
    "            self.labels = None\n",
    "        # ------------------------\n",
    "            \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the lenght of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            length of the dataset (int)\n",
    "        \"\"\"\n",
    "        return len(self.data) if self.data is not None else 0\n",
    "\n",
    "    \n",
    "    def __getitem__(\n",
    "          self\n",
    "        , idx: int\n",
    "        ) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieve the item of the dataset at index idx.\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of the item to be retrieved.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, label) \n",
    "        \"\"\"\n",
    "        img, label = self.data[idx], int(self.labels[idx])\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"\n",
    "        Save the dataset (tuple of torch.tensors) into a file defined by self.processed_folder and self.save_file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.processed_folder):   \n",
    "            os.makedirs(self.processed_folder)  \n",
    "\n",
    "        # saving the training set into the correct folder\n",
    "        with open(os.path.join(self.processed_folder, self.save_file), 'wb') as f:\n",
    "            torch.save((self.data, self.labels), f)\n",
    "\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the file .pt in the path defined by self.processed_folder and self.save_file.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.processed_folder, self.save_file)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(\"Folder not present: {}\".format(file_path))\n",
    "\n",
    "        self.data, self.labels = torch.load(file_path)\n",
    "\n",
    "    \n",
    "    def splits(\n",
    "          self\n",
    "        , proportions: list=[0.8, 0.2]\n",
    "        , shuffle: bool=True\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Split the the dataset according to the given proportions and return two instances of MNIST, training and validation.\n",
    "\n",
    "        Args:\n",
    "            proportions (list): (default=[0.8,0.2]) list of proportions for training set and validation set.\n",
    "            shuffle (bool): (default=True) whether to shuffle the dataset or not\n",
    "        \"\"\"\n",
    "\n",
    "        # check proportions\n",
    "        if sum(proportions) == 1. and all([p > 0. for p in proportions]) and len(proportions) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid proportions: they must (1) be 2 (2) sum up to 1 (3) be all positives.\")\n",
    "\n",
    "        \n",
    "        # creating data indices for training and validation splits\n",
    "        length = self.len()\n",
    "        if length == 0:\n",
    "            raise ValueError(\"Dataset must NOT be empty!\")\n",
    "        indices = np.arange(length)\n",
    "        split = int(np.floor(proportions[0] * length))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)  # in-place operation\n",
    "        \n",
    "        training_indices, validation_indices = indices[:split], indices[split:]\n",
    "\n",
    "        # dividing data with respect to the main classes\n",
    "        data_per_class = {}\n",
    "        for j in range(0, self.main_class_count):\n",
    "            data_per_class.append([])\n",
    "\n",
    "        for i in range(0, len(self.files)):\n",
    "            data_per_class[self.labels[i]].append(i)\n",
    "\n",
    "        num_splits = len(proportions)\n",
    "\n",
    "        # creating a list of Dataset object (one dataset for each split)\n",
    "        datasets = []\n",
    "        for i in range(0, num_splits):\n",
    "            datasets.append(Dataset(self.path, empty_dataset=True))\n",
    "\n",
    "        # splitting data\n",
    "        for j in range(0, self.main_class_count):\n",
    "            start = 0  # index of the first element to consider\n",
    "\n",
    "            for i in range(0, num_splits):\n",
    "                p = proportions[i]\n",
    "                n = int(p * len(data_per_class[j]))  # number of element to consider for class 'j'\n",
    "                end = start + n if i < num_splits - 1 else len(data_per_class[j])  # last (excluded) element to consider\n",
    "\n",
    "                sample_ids = data_per_class[j][start:end]  # indices of the samples to consider\n",
    "\n",
    "                # adding the selected data to the current split\n",
    "                datasets[i].files.extend([self.files[z] for z in sample_ids])\n",
    "                datasets[i].labels.extend([self.labels[z] for z in sample_ids])\n",
    "                if len(self.images) > 0:\n",
    "                    datasets[i].images.extend([self.images[z] for z in sample_ids])\n",
    "\n",
    "                start = end  # moving the starting index\n",
    "\n",
    "        return datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "# prova = MNIST(path, train=True, download=True, empty=False)\n",
    "prova = MNIST(path, train=True, download=False, empty=True)\n",
    "prova.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}