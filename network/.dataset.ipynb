{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitf5e98526d62c4af1bdda9906588672b8",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MNIST dataset handling\n",
    "---\n",
    "\n",
    "In this notebook we will deploy a custom `dataset` class which will be able to:\n",
    "- import the MNIST dataset from a **url**\n",
    "- **read** the MNIST dataset and **load** it in a `torch.tensor`\n",
    "- **save** the dataset in `.pt` format to be easily accessible within the `PyTorch` environment\n",
    "- provide a method to create the dataset **splits**, according to some proportions\n",
    "- provide a method to perform some **preprocessing** operations\n",
    "\n",
    "We will procede as follows:\n",
    "- file decoding procedure\n",
    "    - analisys of the MNIST dataset format (info taken from this [source](http://yann.lecun.com/exdb/mnist/))\n",
    "    - reading the file and retrieving the data dimensions and type\n",
    "    - loading the data into a `torch.tensor`\n",
    "- `dataset` class construction\n",
    "    - `__init__`\n",
    "    - `create_splits`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct       # to unpack binary data https://docs.python.org/3/library/struct.html"
   ]
  },
  {
   "source": [
    "## File decoding procedure\n",
    "### MNIST Dataset format\n",
    "\n",
    "\n",
    "The **IDX file format** is a simple format for vectors and multidimensional matrices of various numerical types.\n",
    "The basic format is\n",
    "\n",
    "```\n",
    "magic number\n",
    "size in dimension 0\n",
    "size in dimension 1\n",
    "size in dimension 2\n",
    ".....\n",
    "size in dimension N\n",
    "data\n",
    "```\n",
    "    \n",
    "The magic number is an integer (MSB first). The first 2 bytes are always 0.\n",
    "\n",
    "The third byte codes the type of the data:\n",
    "- 0x08: unsigned byte\n",
    "- 0x09: signed byte\n",
    "- 0x0B: short (2 bytes)\n",
    "- 0x0C: int (4 bytes)\n",
    "- 0x0D: float (4 bytes)\n",
    "- 0x0E: double (8 bytes)\n",
    "\n",
    "The 4-th byte codes the number of dimensions of the vector/matrix: 1 for vectors, 2 for matrices....\n",
    "\n",
    "The sizes in each dimension are 4-byte integers (MSB first, high endian, like in most non-Intel processors).\n",
    "\n",
    "The data is stored like in a C array, i.e. the index in the last dimension changes the fastest."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Reading the file and retrieving the data dimensions and type\n",
    "\n",
    "Taking into account the MNIST dataset format provided in the reference at the beginning of this notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-images-idx3-ubyte', 'rb') as f:    # open the file for reading in binary mode 'rb'                         \n",
    "    m_numb_32bit = f.read(4)    # magic number"
   ]
  },
  {
   "source": [
    "At the moment, all the retrieved informations are in the form of bytes. \n",
    "\n",
    "The `f.read(4)` function call, reads 4 bytes (32 bits) at a time from the original file, then it proceeds to the next 4. And so on.\n",
    "\n",
    "In the above variable, the information are stored in exadecimal binary format.\n",
    "\n",
    "We can then retrieve the bytes composing the magic number and store them in a list (`m_numb_list`) in which the indices are as follows:\n",
    " - \\[0\\]: 0\n",
    " - \\[1\\]: 0\n",
    " - \\[2\\]: encoding number for the type of the data\n",
    " - \\[3\\]: number of dimensions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nm_numb_list:  [0, 0, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "m_numb_list = [byte for byte in m_numb_32bit]\n",
    "print('\\nm_numb_list: ',m_numb_list)"
   ]
  },
  {
   "source": [
    "As anticipated, the bytes of the magic number gives us some information. In particular:\n",
    "- the third byte defines the type of the data (in this case it is `0x08` which encoded for `unsigned byte`)\n",
    "- the fourth byte defines the number of dimensions (in this case `3`, so we have a cube)\n",
    "\n",
    "We can then read the following bytes to retrieve some other information about the dimensions of the data:\n",
    "- d_list_32bit\\[0\\]: size in dimension 0\n",
    "- d_list_32bit\\[1\\]: size in dimension 1\n",
    "- d_list_32bit\\[2\\]: size in dimension 2\n",
    "- ...\n",
    "- d_list_32bit\\[N\\]: size in dimension N\n",
    "\n",
    "**NOTE**: here we are considering the `train-images-idx3-ubyte` file, which is referred to the training images. For the other files, more or less dimensions might be available."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d_list_32bit: [b'\\x00\\x00\\xea`', b'\\x00\\x00\\x00\\x1c', b'\\x00\\x00\\x00\\x1c']\n"
     ]
    }
   ],
   "source": [
    "with open('train-images-idx3-ubyte', 'rb') as f:    # open the file for reading in binary mode 'rb'       \n",
    "    f.read(4)                   # discard the first 4 bytes (magic number)\n",
    "    d_list_32bit = [f.read(4) for _ in range(m_numb_list[3])]\n",
    "\n",
    "print('d_list_32bit:', d_list_32bit)"
   ]
  },
  {
   "source": [
    "We obtain a list of three elements, which represents the the number of data in each dimension.\n",
    "\n",
    "We use the `struct` module from Python in order to convert the byte format into the decimal one.\n",
    "In order to do so, we use the big-endian format `\">\"` along with the type/size of bytes that we want to unpack at a time: \n",
    "- for the informations bytes (which corresponds to the N + 1 first 4-bytes (32 bits) elements of the file, being N the number of dimensions) we use the `\">I\"` format (i.e. big-endian 4-bytes int)\n",
    "- for the actual data we will rely on the third byte of the magic number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dimensions: [60000, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "dimensions = [struct.unpack('>I', dimension)[0] for dimension in d_list_32bit]\n",
    "print('dimensions:', dimensions)\n"
   ]
  },
  {
   "source": [
    "At this point we can wrap all the lines of code written so far into a prototype function which will read an input IDX file and return the list of its dimensions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx_file(file_path: str) -> list:\n",
    "    \n",
    "    # open the file for reading in binary mode 'rb'\n",
    "    with open(file_path, 'rb') as f:     \n",
    "        # magic number list   \n",
    "        m_numb_list = [byte for byte in f.read(4)] \n",
    "        # dimensions list  \n",
    "        d_list_32bit = [f.read(4) for _ in range(m_numb_list[3])]\n",
    "        # unpack data\n",
    "        dimensions = [struct.unpack('>I', dimension)[0] for dimension in d_list_32bit]\n",
    "\n",
    "        return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[60000, 28, 28]\n[60000]\n[10000, 28, 28]\n[10000]\n"
     ]
    }
   ],
   "source": [
    "print(read_idx_file(r'train-images-idx3-ubyte'))\n",
    "print(read_idx_file(r'train-labels-idx1-ubyte'))\n",
    "print(read_idx_file(r't10k-images-idx3-ubyte'))\n",
    "print(read_idx_file(r't10k-labels-idx1-ubyte'))"
   ]
  },
  {
   "source": [
    "### Loading the data into a `torch.tensor`\n",
    "\n",
    "Now we can focus on the part of the _file decoding procedure_: retrieving the data and loading it into a `torch.tensor`.\n",
    "\n",
    "It will be useful to report the encoding strings here and to create an ad-hoc dictionary to handle it.\n",
    "- 0x08: unsigned byte\n",
    "- 0x09: signed byte\n",
    "- 0x0B: short (2 bytes)\n",
    "- 0x0C: int (4 bytes)\n",
    "- 0x0D: float (4 bytes)\n",
    "- 0x0E: double (8 bytes)\n",
    "\n",
    "The encoding dictionary contains, for each exadecimal byte, the corresponding format, the standard size (used by `f.read(...)` in order to match the size defined by the format) and the PyTorch type, to be used when we will load the dataset into a `torch.tensor`.\n",
    "\n",
    "The format that we will use to unpack the data is reported below.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "      b'\\x08':['B',1,torch.uint8]\n",
    "    , b'\\x09':['b',1,torch.int8]\n",
    "    , b'\\x0B':['h',2,torch.short]\n",
    "    , b'\\x0C':['i',4,torch.int32]\n",
    "    , b'\\x0D':['f',4,torch.float32]\n",
    "    , b'\\x0E':['d',8,torch.float64]\n",
    "    }\n",
    "\n",
    "e_format = \">\" + encoding[m_numb_list[2].to_bytes(1, byteorder='big')][0]\n",
    "n_bytes = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][1]\n",
    "d_type = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][2]"
   ]
  },
  {
   "source": [
    "Now that we have the dimensions and the encoding format we can procede retrieving the actual data.\n",
    "To do so, we will consider again the case of the training images dataset trying to generalize it as much as possible, in order to exploit the code for the other files too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading time: 19.934138\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with open('train-images-idx3-ubyte', 'rb') as f:    # open the file for reading in binary mode 'rb'       \n",
    "    f.read(16)    # discard the first 16 bytes (magic number + informations)\n",
    "    \n",
    "    # reading all the bytes of the file progressively and store them in a torch.tensor accordingly to the dimensions\n",
    "    dataset = torch.tensor([[[struct.unpack(e_format, f.read(n_bytes))[0] \n",
    "                                for _ in range(dimensions[2])] \n",
    "                                for _ in range(dimensions[1])] \n",
    "                                for _ in range(dimensions[0])]\n",
    "                            , dtype=d_type)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Loading time: {:2f}\".format(end-start))\n"
   ]
  },
  {
   "source": [
    "The bigger dataset, which in this case is the one of the training images, takes approximately 20s to be read and loaded into a `torch.tensor`.\n",
    "\n",
    "We can update the previously defined function, changing a bit its name into `read_idx_file_to_tensor()` and adapting the tensor inizialization taking into account the possibility of having different dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx_file_to_tensor(file_path: str) -> torch.tensor:\n",
    "    \n",
    "    # open the file for reading in binary mode 'rb'\n",
    "    with open(file_path, 'rb') as f:     \n",
    "        # magic number list   \n",
    "        m_numb_list = [byte for byte in f.read(4)] \n",
    "        # dimensions list  \n",
    "        d_list_32bit = [f.read(4) for _ in range(m_numb_list[3])]\n",
    "        dimensions = [struct.unpack('>I', dimension)[0] for dimension in d_list_32bit]\n",
    "        \n",
    "        encoding = {\n",
    "                      b'\\x08':['B',1,torch.uint8]\n",
    "                    , b'\\x09':['b',1,torch.int8]\n",
    "                    , b'\\x0B':['h',2,torch.short]\n",
    "                    , b'\\x0C':['i',4,torch.int32]\n",
    "                    , b'\\x0D':['f',4,torch.float32]\n",
    "                    , b'\\x0E':['d',8,torch.float64]\n",
    "                    }\n",
    "\n",
    "        e_format = \">\" + encoding[m_numb_list[2].to_bytes(1, byteorder='big')][0]\n",
    "        n_bytes = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][1]\n",
    "        d_type = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][2]\n",
    "\n",
    "\n",
    "        if len(dimensions) == 3:    # images\n",
    "           \n",
    "            print('Loading {} ...'.format(file_path))\n",
    "            \n",
    "            dataset = torch.tensor(\n",
    "                [\n",
    "                    [\n",
    "                        [struct.unpack(e_format, f.read(n_bytes))[0] \n",
    "                        for _ in range(dimensions[2])] \n",
    "                    for _ in range(dimensions[1])] \n",
    "                for _ in range(dimensions[0])]\n",
    "                , dtype=d_type\n",
    "            )\n",
    "\n",
    "            print('{} loaded!'.format(file_path))\n",
    "        \n",
    "\n",
    "        elif len(dimensions) == 1:  # labels\n",
    "        \n",
    "            print('Loading {} ...'.format(file_path))\n",
    "\n",
    "            dataset = torch.tensor(\n",
    "                [struct.unpack(e_format, f.read(n_bytes))[0]\n",
    "                for _ in range(dimensions[0])]\n",
    "                , dtype=d_type\n",
    "            )\n",
    "\n",
    "            print('{} loaded!'.format(file_path))\n",
    "        \n",
    "\n",
    "        else:   # wrong dimensions\n",
    "            raise ValueError(\"Invalid dimensions in the IDX file!\")\n",
    "\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "source": [
    "Having created this main function, we are able to call it, passing the paths to the files. One possible method consists in calling the training images and the training labels inside a tuple called `training_set` which will then be just a tuple of `torch.tensor`s. The same will be done for the `test_set` tuple, as shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading train-images-idx3-ubyte ...\n",
      "train-images-idx3-ubyte loaded!\n",
      "Loading train-labels-idx1-ubyte ...\n",
      "train-labels-idx1-ubyte loaded!\n",
      "Loading t10k-images-idx3-ubyte ...\n",
      "t10k-images-idx3-ubyte loaded!\n",
      "Loading t10k-labels-idx1-ubyte ...\n",
      "t10k-labels-idx1-ubyte loaded!\n"
     ]
    }
   ],
   "source": [
    "training_set = (\n",
    "    read_idx_file_to_tensor(r'train-images-idx3-ubyte')\n",
    "    , read_idx_file_to_tensor(r'train-labels-idx1-ubyte')\n",
    ")\n",
    "\n",
    "test_set = (\n",
    "    read_idx_file_to_tensor(r't10k-images-idx3-ubyte')\n",
    "    , read_idx_file_to_tensor(r't10k-labels-idx1-ubyte')\n",
    "\n",
    ")"
   ]
  }
 ]
}