{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitf5e98526d62c4af1bdda9906588672b8",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MNIST dataset handling\n",
    "---\n",
    "\n",
    "In this notebook we will develop a custom `dataset` class which will be able to:\n",
    "- import the MNIST dataset from a **url**\n",
    "- **read** the MNIST dataset and **load** it in a `torch.tensor`\n",
    "- **save** the dataset in `.pt` format to be easily accessible within the `PyTorch` environment\n",
    "- provide a method to create the dataset **splits**, according to some proportions\n",
    "- provide a method to perform some **preprocessing** operations\n",
    "\n",
    "We will procede as follows:\n",
    "- file decoding procedure\n",
    "    - analisys of the MNIST dataset format (info taken from this [source](http://yann.lecun.com/exdb/mnist/))\n",
    "    - download the files from the sources\n",
    "    - reading the file and retrieving the data dimensions and type\n",
    "    - loading the data into a `torch.tensor`\n",
    "- `dataset` class construction\n",
    "    - define a constructor `__init__`\n",
    "    - provide a method `create_splits` to split the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## File decoding procedure\n",
    "### MNIST Dataset format\n",
    "\n",
    "\n",
    "The **IDX file format** is a simple format for vectors and multidimensional matrices of various numerical types.\n",
    "The basic format is\n",
    "\n",
    "```\n",
    "magic number\n",
    "size in dimension 0\n",
    "size in dimension 1\n",
    "size in dimension 2\n",
    ".....\n",
    "size in dimension N\n",
    "data\n",
    "```\n",
    "    \n",
    "The magic number is an integer (MSB first). The first 2 bytes are always 0.\n",
    "\n",
    "The third byte codes the type of the data:\n",
    "- 0x08: unsigned byte\n",
    "- 0x09: signed byte\n",
    "- 0x0B: short (2 bytes)\n",
    "- 0x0C: int (4 bytes)\n",
    "- 0x0D: float (4 bytes)\n",
    "- 0x0E: double (8 bytes)\n",
    "\n",
    "The 4-th byte codes the number of dimensions of the vector/matrix: 1 for vectors, 2 for matrices....\n",
    "\n",
    "The sizes in each dimension are 4-byte integers (MSB first, high endian, like in most non-Intel processors).\n",
    "\n",
    "The data is stored like in a C array, i.e. the index in the last dimension changes the fastest."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### File downloading from url"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The MNIST dataset is freely available at this [source](http://yann.lecun.com/exdb/mnist/).\n",
    "Our aim is that of automatically downloading it (if not present in the project folders) and save it in order to let it be accessible from the `dataset` class that we are going to develop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "          'training-images': 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "        , 'training-labels': 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "        , 'test-images': 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "        , 'test-labels': 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "}"
   ]
  },
  {
   "source": [
    "In order to download something from a url we can take advantage of the [requests](https://2.python-requests.org/en/master/api/) library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "source": [
    "First of all we need to know whether the sources are _downloadable_ or not. This is just a check and can be easily implemented as follows:\n",
    "- we look at the `content-type` of the document: if it is **application/x-gzip**, that it is a valid source for our dataset.\n",
    "\n",
    "To do so, we can use the `requests.head` function, which is useful for obtaining met-ainformation about the entity implied by the request without transferring the entity-body itself."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, url in urls.items():\n",
    "    h = requests.head(url)\n",
    "    print(\"{} -> {}\".format(name, h.headers['content-type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_downloadable(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Does the url contain a downloadable resource for our project.\n",
    "\n",
    "    Args:\n",
    "        url     (str): url of the source to be downloaded\n",
    "\n",
    "    Returns:\n",
    "        is_downloadable (bool): True if the source has application/x-gzip as content-type \n",
    "    \"\"\"\n",
    "    h = requests.head(url, allow_redirects=True)\n",
    "    header = h.headers\n",
    "    content_type = header.get('content-type')\n",
    "    if 'application/x-gzip' in content_type.lower():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, url in urls.items():\n",
    "    print(\"{} -> {}\".format(name, is_downloadable(url)))"
   ]
  },
  {
   "source": [
    "In order to write the content of the file to be downloaded in a file located in our machine, we have to retrieve the filename. \n",
    "\n",
    "For instance: `http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz` gives us the filename `t10k-labels-idx1-ubyte.gz`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the filename\n",
    "for name, url in urls.items():\n",
    "    if url.find('/'):\n",
    "        zipped_filename = url.rsplit('/', 1)[-1]\n",
    "        if zipped_filename.find('.'):\n",
    "            filename = zipped_filename.rsplit('.', 1)[-2]\n",
    "        print(\"{} -> {} | {}\".format(name, zipped_filename, filename))"
   ]
  },
  {
   "source": [
    "Once we have verified the correctness of the files to be downloaded and retrieved the filename, we can proceed to download the actual content and write it into a file in our machine. \n",
    "\n",
    "For this purpose, we can take advantage of the `request.get` function which, compared to the `request.head` function, returns us also the body information (i.e. the actual data we want to download).\n",
    "Once the filenames are available we can save the content into them.\n",
    "\n",
    "In the next cell, what obtain a `request.Response` object, whose content is the data that we are going to use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file content\n",
    "r = requests.get(urls['test-labels'], stream=True)\n",
    "r.raw.read(10)  # read raw content (not decoded yet)\n"
   ]
  },
  {
   "source": [
    "Then we can write this content into the actual file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file content\n",
    "r = requests.get(urls['test-labels'], stream=True)\n",
    "with open('test.gz', 'wb') as f:\n",
    "    f.write(r.raw.data)"
   ]
  },
  {
   "source": [
    "Wrapping up the lines of code, we came up with the following function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a .gz file from the provided url and saves it to a folder.\n",
    "\n",
    "    Args:\n",
    "        url         (str): url of the source to be downloaded\n",
    "        folder      (str): folder in which the file will be saved\n",
    "    \"\"\"\n",
    "    if is_downloadable(url):\n",
    "        # retrieve the filename\n",
    "        for name, url in urls.items():\n",
    "            if url.find('/'):\n",
    "                zipped_filename = url.rsplit('/', 1)[-1]\n",
    "                if zipped_filename.find('.'):\n",
    "                    filename = zipped_filename.rsplit('.', 1)[-2]\n",
    "\n",
    "        # get the file content\n",
    "        r = requests.get(url, stream=True)\n",
    "\n",
    "        # write the files\n",
    "        with open(\"{}{}\".format(folder, zipped_filename), 'wb') as f:\n",
    "            f.write(r.raw.data)\n"
   ]
  },
  {
   "source": [
    "The following step regards the unzipping procedure. To do so, we can exploit the [gzip](https://docs.python.org/3/library/gzip.html) library and, in particular, the `gzip.open` function along with the [shutil](https://docs.python.org/3/library/shutil.html) library with the `shutil.copyfileobj` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# open the zipped file\n",
    "with gzip.open('test.gz', 'rb') as f_in:\n",
    "    # open the uncompressed file to be filled\n",
    "    with open('test', 'wb') as f_out:\n",
    "        # fill the uncompressed file\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Before completining the download function, we should include some checks regarding the existance of the directory in which we are going to save the files.\n",
    "\n",
    "Thus, we have to import the [os](https://docs.python.org/3/library/os.html) library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = './data/'\n",
    "\n",
    "if not os.path.exists(folder):   # check for the existence of directory ./data/\n",
    "    os.makedirs(folder)          # creation of the folder if it doesn't exist"
   ]
  },
  {
   "source": [
    "We are ready to build our `download` function taking care of checking for the presence of already downloaded files (In that case we will skip the download)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a .gz file from the provided url and saves it to a folder.\n",
    "\n",
    "    Args:\n",
    "        url         (str): url of the source to be downloaded\n",
    "        folder      (str): folder in which the file will be saved\n",
    "    \"\"\"\n",
    "    if is_downloadable(url):\n",
    "        # retrieve the filename\n",
    "        if url.find('/'):\n",
    "            compressed_filename = url.rsplit('/', 1)[-1]\n",
    "            if compressed_filename.find('.'):\n",
    "                filename = compressed_filename.rsplit('.', 1)[-2]\n",
    "\n",
    "        # get the file content\n",
    "        r = requests.get(url, stream=True)\n",
    "\n",
    "        # check for the existence of directory\n",
    "        if not os.path.exists(folder):   \n",
    "            # creation of the folder if it doesn't exist\n",
    "            os.makedirs(folder)          \n",
    "\n",
    "        compressed_file_path = os.path.join(folder, compressed_filename)\n",
    "        file_path = os.path.join(folder, filename)\n",
    "\n",
    "        print(\"Checking presence of {} ...\".format(compressed_file_path))\n",
    "        if not os.path.exists(compressed_file_path): # if no downloads present\n",
    "\n",
    "            print(\"Downloading {} ...\".format(compressed_file_path))\n",
    "            \n",
    "            # write the files\n",
    "            with open(compressed_file_path, 'wb') as f:\n",
    "                f.write(r.raw.data)\n",
    "        \n",
    "        else:   # if the files have already been downloaded\n",
    "            print(\"Already downloaded.\")\n",
    "\n",
    "\n",
    "        print(\"Checking presence of uncompressed file {} ...\".format(file_path))\n",
    "        if not os.path.exists(file_path):   # if uncompressed file is present \n",
    "\n",
    "            print(\"Extracting {} ...\".format(file_path))\n",
    "            \n",
    "            # open the compressed file\n",
    "            with gzip.open(compressed_file_path, 'rb') as f_in:\n",
    "                # open the uncompressed file to be filled\n",
    "                with open(file_path, 'wb') as f_out:\n",
    "                    # fill the uncompressed file\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        else:   # if the files have already been downloaded\n",
    "            print(\"Already downloaded.\")\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "source": [
    "Now we can easily download the files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, url in urls.items():\n",
    "    download(url, './data/raw')"
   ]
  },
  {
   "source": [
    "### Reading and information retrieving\n",
    "\n",
    "Taking into account the MNIST dataset format provided in the reference at the beginning of this notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test', 'rb') as f:    # open the file for reading in binary mode 'rb'                         \n",
    "    m_numb_32bit = f.read(4)    # magic number\n",
    "    print(m_numb_32bit)"
   ]
  },
  {
   "source": [
    "At the moment, all the retrieved informations are in the form of bytes. \n",
    "\n",
    "The `f.read(4)` function call, reads 4 bytes (32 bits) at a time from the original file, then it proceeds to the next 4. And so on.\n",
    "\n",
    "In the above variable, the information are stored in exadecimal binary format.\n",
    "\n",
    "We can then retrieve the bytes composing the magic number and store them in a list (`m_numb_list`) in which the indices are as follows:\n",
    " - \\[0\\]: 0\n",
    " - \\[1\\]: 0\n",
    " - \\[2\\]: encoding number for the type of the data\n",
    " - \\[3\\]: number of dimensions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_numb_list = [byte for byte in m_numb_32bit]\n",
    "print('\\nm_numb_list: ',m_numb_list)"
   ]
  },
  {
   "source": [
    "As anticipated, the bytes of the magic number gives us some information. In particular:\n",
    "- the third byte defines the type of the data (in this case it is `0x08` which encoded for `unsigned byte`)\n",
    "- the fourth byte defines the number of dimensions (in this case `3`, so we have a cube)\n",
    "\n",
    "We can then read the following bytes to retrieve some other information about the dimensions of the data:\n",
    "- d_list_32bit\\[0\\]: size in dimension 0\n",
    "- d_list_32bit\\[1\\]: size in dimension 1\n",
    "- d_list_32bit\\[2\\]: size in dimension 2\n",
    "- ...\n",
    "- d_list_32bit\\[N\\]: size in dimension N\n",
    "\n",
    "**NOTE**: here we are considering the `train-images-idx3-ubyte` file, which is referred to the training images. For the other files, more or less dimensions might be available."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-images-idx3-ubyte', 'rb') as f:    # open the file for reading in binary mode 'rb'       \n",
    "    f.read(4)                                       # discard the first 4 bytes (magic number)\n",
    "    d_list_32bit = [f.read(4) for _ in range(m_numb_list[3])]\n",
    "\n",
    "print('d_list_32bit:', d_list_32bit)"
   ]
  },
  {
   "source": [
    "We obtain a list of three elements, which represents the the number of data in each dimension.\n",
    "\n",
    "We use the `struct` module from Python in order to convert the byte format into the decimal one.\n",
    "In order to do so, we use the big-endian format `\">\"` along with the type/size of bytes that we want to unpack at a time: \n",
    "- for the informations bytes (which corresponds to the N + 1 first 4-bytes (32 bits) elements of the file, being N the number of dimensions) we use the `\">I\"` format (i.e. big-endian 4-bytes int)\n",
    "- for the actual data we will rely on the third byte of the magic number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [struct.unpack('>I', dimension)[0] for dimension in d_list_32bit]\n",
    "print('dimensions:', dimensions)\n"
   ]
  },
  {
   "source": [
    "At this point we can wrap all the lines of code written so far into a prototype function which will read an input IDX file and return the list of its dimensions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx_file(file_path: str) -> list:\n",
    "    \n",
    "    # open the file for reading in binary mode 'rb'\n",
    "    with open(file_path, 'rb') as f:     \n",
    "        # magic number list   \n",
    "        m_numb_list = [byte for byte in f.read(4)] \n",
    "        # dimensions list  \n",
    "        d_list_32bit = [f.read(4) for _ in range(m_numb_list[3])]\n",
    "        # unpack data\n",
    "        dimensions = [struct.unpack('>I', dimension)[0] for dimension in d_list_32bit]\n",
    "\n",
    "        return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_idx_file(r'train-images-idx3-ubyte'))\n",
    "print(read_idx_file(r'train-labels-idx1-ubyte'))\n",
    "print(read_idx_file(r't10k-images-idx3-ubyte'))\n",
    "print(read_idx_file(r't10k-labels-idx1-ubyte'))"
   ]
  },
  {
   "source": [
    "### Loading the data into a `torch.tensor`\n",
    "\n",
    "Now we can focus on the part of the _file decoding procedure_: retrieving the data and loading it into a `torch.tensor`.\n",
    "\n",
    "It will be useful to report the encoding strings here and to create an ad-hoc dictionary to handle it.\n",
    "- 0x08: unsigned byte\n",
    "- 0x09: signed byte\n",
    "- 0x0B: short (2 bytes)\n",
    "- 0x0C: int (4 bytes)\n",
    "- 0x0D: float (4 bytes)\n",
    "- 0x0E: double (8 bytes)\n",
    "\n",
    "The encoding dictionary contains, for each exadecimal byte, the corresponding format, the standard size (used by `f.read(...)` in order to match the size defined by the format) and the PyTorch type, to be used when we will load the dataset into a `torch.tensor`.\n",
    "\n",
    "The format that we will use to unpack the data is reported below.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "      b'\\x08':['B',1,torch.uint8]\n",
    "    , b'\\x09':['b',1,torch.int8]\n",
    "    , b'\\x0B':['h',2,torch.short]\n",
    "    , b'\\x0C':['i',4,torch.int32]\n",
    "    , b'\\x0D':['f',4,torch.float32]\n",
    "    , b'\\x0E':['d',8,torch.float64]\n",
    "    }\n",
    "\n",
    "e_format = \">\" + encoding[m_numb_list[2].to_bytes(1, byteorder='big')][0]\n",
    "n_bytes = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][1]\n",
    "d_type = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][2]"
   ]
  },
  {
   "source": [
    "Now that we have the dimensions and the encoding format we can procede retrieving the actual data.\n",
    "To do so, we will consider again the case of the training images dataset trying to generalize it as much as possible, in order to exploit the code for the other files too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with open('train-images-idx3-ubyte', 'rb') as f:    # open the file for reading in binary mode 'rb'       \n",
    "    f.read(16)                                      # discard the first 16 bytes (magic number + informations)\n",
    "    \n",
    "    # reading all the bytes of the file progressively and store them in a torch.tensor accordingly to the dimensions\n",
    "    dataset = torch.tensor([[[struct.unpack(e_format, f.read(n_bytes))[0] \n",
    "                                for _ in range(dimensions[2])] \n",
    "                                for _ in range(dimensions[1])] \n",
    "                                for _ in range(dimensions[0])]\n",
    "                            , dtype=d_type)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Loading time: {:2f}\".format(end-start))\n"
   ]
  },
  {
   "source": [
    "The bigger dataset, which in this case is the one of the training images, takes approximately 20s to be read and loaded into a `torch.tensor`.\n",
    "\n",
    "We can update the previously defined function, changing a bit its name into `read_idx_file_to_tensor()` and adapting the tensor inizialization taking into account the possibility of having different dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx_file_to_tensor(file_path: str) -> torch.tensor:\n",
    "    \n",
    "    # open the file for reading in binary mode 'rb'\n",
    "    with open(file_path, 'rb') as f:     \n",
    "        # magic number list   \n",
    "        m_numb_list = [byte for byte in f.read(4)] \n",
    "        # dimensions list  \n",
    "        d_list_32bit = [f.read(4) for _ in range(m_numb_list[3])]\n",
    "        dimensions = [struct.unpack('>I', dimension)[0] for dimension in d_list_32bit]\n",
    "        \n",
    "        encoding = {\n",
    "                      b'\\x08':['B',1,torch.uint8]\n",
    "                    , b'\\x09':['b',1,torch.int8]\n",
    "                    , b'\\x0B':['h',2,torch.short]\n",
    "                    , b'\\x0C':['i',4,torch.int32]\n",
    "                    , b'\\x0D':['f',4,torch.float32]\n",
    "                    , b'\\x0E':['d',8,torch.float64]\n",
    "                    }\n",
    "\n",
    "        e_format = \">\" + encoding[m_numb_list[2].to_bytes(1, byteorder='big')][0]\n",
    "        n_bytes = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][1]\n",
    "        d_type = encoding[m_numb_list[2].to_bytes(1, byteorder='big')][2]\n",
    "\n",
    "\n",
    "        if len(dimensions) == 3:    # images\n",
    "           \n",
    "            print('Loading {} ...'.format(file_path))\n",
    "            \n",
    "            dataset = torch.tensor(\n",
    "                [\n",
    "                    [\n",
    "                        [struct.unpack(e_format, f.read(n_bytes))[0] \n",
    "                        for _ in range(dimensions[2])] \n",
    "                    for _ in range(dimensions[1])] \n",
    "                for _ in range(dimensions[0])]\n",
    "                , dtype=d_type\n",
    "            )\n",
    "\n",
    "            print('{} loaded!'.format(file_path))\n",
    "        \n",
    "\n",
    "        elif len(dimensions) == 1:  # labels\n",
    "        \n",
    "            print('Loading {} ...'.format(file_path))\n",
    "\n",
    "            dataset = torch.tensor(\n",
    "                [struct.unpack(e_format, f.read(n_bytes))[0]\n",
    "                for _ in range(dimensions[0])]\n",
    "                , dtype=d_type\n",
    "            )\n",
    "\n",
    "            print('{} loaded!'.format(file_path))\n",
    "        \n",
    "\n",
    "        else:   # wrong dimensions\n",
    "            raise ValueError(\"Invalid dimensions in the IDX file!\")\n",
    "\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "source": [
    "Having created this main function, we are able to call it, passing the paths to the files. One possible method consists in calling the training images and the training labels inside a tuple called `training_set` which will then be just a tuple of `torch.tensor`s. The same will be done for the `test_set` tuple, as shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = (\n",
    "    read_idx_file_to_tensor(r'train-images-idx3-ubyte')\n",
    "    , read_idx_file_to_tensor(r'train-labels-idx1-ubyte')\n",
    ")\n",
    "\n",
    "test_set = (\n",
    "    read_idx_file_to_tensor(r't10k-images-idx3-ubyte')\n",
    "    , read_idx_file_to_tensor(r't10k-labels-idx1-ubyte')\n",
    "\n",
    ")"
   ]
  },
  {
   "source": [
    "We can check the correctness of this function by printing the first 10 image-label pairs of each dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_images = 10\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "for i in range(n_images):\n",
    "    image = training_set[0][i]\n",
    "    label = training_set[1][i].item()\n",
    "    sp = fig.add_subplot(2, 5, i+1)\n",
    "    sp.set_title(label)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "for i in range(n_images):\n",
    "    image = test_set[0][i]\n",
    "    label = test_set[1][i].item()\n",
    "    sp = fig.add_subplot(2, 5, i+1)\n",
    "    sp.set_title(label)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}